version: '3'

services:
  nginx:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - backend
    restart: unless-stopped
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
      - /opt/certs:/certs:ro
    
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - JAVA_OPTS=-Xms512m -Xmx1g -XX:+UseG1GC -XX:MaxGCPauseMillis=200
    env_file:
      - ./.env
    volumes:
      - /home/ubuntu/imgs/stock_images:/app/uploads
    restart: unless-stopped
    depends_on:
      - mysql
      - redis
  
  selenium-hub:
    image: selenium/hub:4.15.0
    container_name: selenium-hub
    ports:
      - "4444:4444"
    environment:
      - GRID_MAX_SESSION=3
      - GRID_BROWSER_TIMEOUT=180000
      - GRID_TIMEOUT=180

  chrome:
    image: selenium/node-chrome:4.15.0
    shm_size: 4gb
    depends_on:
      - selenium-hub
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
      - SE_NODE_MAX_SESSIONS=1
      - NODE_MAX_INSTANCES=1
      - SE_NODE_SESSION_TIMEOUT=600
      - SE_NODE_GRID_URL=http://selenium-hub:4444
      - SE_OPTS=--log-level FINE
    restart: always

  crawl:
    build:
      context: ./crawl
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
    environment:
      - SPRING_PROFILES_ACTIVE=selenium-grid
      - SELENIUM_REMOTE_URL=http://selenium-hub:4444/wd/hub
      - JAVA_OPTS=-Xms1g -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+HeapDumpOnOutOfMemoryError
      - CRAWLER_CONCURRENT_WORKERS=1
      - SPRING_KAFKA_CONSUMER_PROPERTIES_SESSION_TIMEOUT_MS=60000
      - SPRING_KAFKA_CONSUMER_PROPERTIES_HEARTBEAT_INTERVAL_MS=20000
      - SPRING_KAFKA_CONSUMER_PROPERTIES_MAX_POLL_INTERVAL_MS=600000
      - SPRING_KAFKA_CONSUMER_PROPERTIES_MAX_POLL_RECORDS=10
      - SPRING_KAFKA_CONSUMER_PROPERTIES_RECONNECT_BACKOFF_MS=5000
      - SPRING_KAFKA_CONSUMER_PROPERTIES_RECONNECT_BACKOFF_MAX_MS=30000
      - CRAWLER_RATE_LIMIT=5000
    env_file:
      - ./crawl/.env
    depends_on:
      - selenium-hub
      - mysql
      - kafka
      - redis
    restart: always

  mysql:
    image: mysql:8.0.33
    ports:
      - "3306:3306"
    volumes:
      - db-data:/var/lib/mysql
    environment:
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USERNAME}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_INNODB_BUFFER_POOL_SIZE=256M
    restart: unless-stopped

  redis:
    image: redis:7.2.4-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_HEAP_OPTS=-Xmx1g -Xms512m
      - KAFKA_JVM_PERFORMANCE_OPTS=-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:+ExplicitGCInvokesConcurrent
      - KAFKA_NUM_NETWORK_THREADS=6
      - KAFKA_NUM_IO_THREADS=8
      - KAFKA_SOCKET_SEND_BUFFER_BYTES=1048576
      - KAFKA_SOCKET_RECEIVE_BUFFER_BYTES=1048576
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=5000
      - KAFKA_CONNECTIONS_MAX_IDLE_MS=600000
    depends_on:
      - zookeeper
    restart: unless-stopped
    volumes:
      - kafka-data:/var/lib/kafka/data

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
      - JVMFLAGS=-Xmx512m -Xms256m
    ports:
      - "2181:2181"
    restart: unless-stopped
    volumes:
      - zookeeper-data:/var/lib/zookeeper

  ai:
    build:
      context: ./ai
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./scripts:/scripts
    entrypoint: ["/bin/bash", "/scripts/start_and_adjust.sh"]
    environment:
      - MODEL_BATCH_SIZE=4
      - WORKERS=1
      - MAX_CONCURRENT_REQUESTS=4
    restart: unless-stopped

volumes:
  db-data:
  redis-data:
  kafka-data:
  zookeeper-data: