# === í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ===
import logging
import os
from collections import Counter
from contextlib import asynccontextmanager
from typing import List, Dict
import time

# === ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ===
from fastapi import FastAPI, HTTPException, status, Response
from pydantic import BaseModel
from transformers import pipeline
from konlpy.tag import Okt
import torch.nn as nn
import uvicorn
from transformers import logging as hf_logging
hf_logging.set_verbosity(hf_logging.ERROR)


# === ë‚´ë¶€ ëª¨ë“ˆ ===
from utils.model_loader import load_all_models_and_tokenizers
from utils.predictor import predict, compute_article_score, calculate_weighted_article_score
from utils.preprocessor import preprocessing_single_news

# === ì „ì—­ ì„¤ì • ===
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("score_article")

model_dict = {}
summarizer = None
models_loaded = False
okt = Okt()


# === FastAPI lifespan í•¸ë“¤ëŸ¬ ===
@asynccontextmanager
async def lifespan(app: FastAPI):
    global model_dict, summarizer, models_loaded
    try:
        logger.info("ðŸ’¡ ëª¨ë¸ ì„œë¹™ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")

        start_time = time.perf_counter()  # ì‹œìž‘ ì‹œê°„ ì¸¡ì •

        model_dict = load_all_models_and_tokenizers()
        summarizer = pipeline("summarization", model="noahkim/KoT5_news_summarization")
        models_loaded = True

        duration = time.perf_counter() - start_time  # ê²½ê³¼ ì‹œê°„ ê³„ì‚°

        os.makedirs("tmp", exist_ok=True)
        with open("tmp/models_loaded", "w") as f:
            f.write("ok")

        logger.info("âœ… ëª¨ë“  ëª¨ë¸ê³¼ ìš”ì•½ê¸° ë¡œë”© ì™„ë£Œ (â± %.2fì´ˆ ì†Œìš”)", duration)
        yield

    except Exception as e:
        logger.exception("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        raise RuntimeError(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")


# === FastAPI ì•± ìƒì„± ===
app = FastAPI(title="News AI API", lifespan=lifespan)


# === ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜ ===
class ScoreRequest(BaseModel):
    title: str
    content: str

class ScoreResponse(BaseModel):
    content: str
    aspect_scores: Dict[str, float]
    score: float

class SummarizationRequest(BaseModel):
    content: str
    max_length: int = 300
    min_length: int = 40
    do_sample: bool = False

class SummarizationResponse(BaseModel):
    summary_content: str

class KeywordItem(BaseModel):
    word: str
    count: int

class KeywordResponse(BaseModel):
    keywords: List[KeywordItem]

class Article(BaseModel):
    content: str

class KeywordRequest(BaseModel):
    articles: List[Article]


# === API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ===
@app.get("/")
async def home():
    return {"message": "Welcome to the News AI API!"}


@app.get("/health")
def health_check(response: Response):
    if not models_loaded:
        response.status_code = status.HTTP_503_SERVICE_UNAVAILABLE
        return {"status": "unhealthy", "models_loaded": False}
    return {"status": "healthy", "models_loaded": True}


@app.post("/score", response_model=ScoreResponse)
async def score_article(input_data: ScoreRequest):
    try:
        news_dict = {"title": input_data.title, "content": input_data.content}
        processed = preprocessing_single_news(news_dict)

        if not processed:
            return ScoreResponse(content="", aspect_scores={}, score=0.0)


        sentences = processed["filtered_sentences"]
        cleaned_content = processed["cleaned_content"]
        logger.info("ë¬¸ìž¥ ë¦¬ìŠ¤íŠ¸: %s", sentences)

        aspect_scores: Dict[str, List[float]] = {cat: [] for cat in model_dict.keys()}

        # ê° ë¬¸ìž¥ì— ëŒ€í•´ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        for s in sentences:
            sentence = s["sentence"]
            if not sentence:
                continue

            for category in [cat for cat in model_dict if s.get(cat, 0) == 1]:
                model, tokenizer, device = model_dict[category]
                score = compute_article_score(predict(model, tokenizer, sentence, device))[1]
                aspect_scores[category].append(score)

        logger.info("ì¸¡ë©´ ë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸: %s", aspect_scores)

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        average_scores = {
            category: round(sum(scores) / len(scores), 2) if scores else 0.0
            for category, scores in aspect_scores.items()
        }

        # í˜¸/ì•…ìž¬ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê·  ë°©ì‹)
        article_score = calculate_weighted_article_score(aspect_scores)
        logger.info("ê¸°ì‚¬ í˜¸/ì•…ìž¬ ì ìˆ˜: %s", article_score)
        logger.info("ì¸¡ë©´ ë³„ ì ìˆ˜: %s", average_scores)


        return ScoreResponse(content=cleaned_content, aspect_scores=average_scores, score=article_score)

    except Exception as e:
        logger.exception("Error in /score endpoint")
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/summarize", response_model=SummarizationResponse)
def summarize(request: SummarizationRequest):
    try:
        result = summarizer(
            request.content,
            max_length=request.max_length,
            min_length=request.min_length,
            do_sample=request.do_sample
        )
        return SummarizationResponse(summary_content=result[0]["summary_text"])
    except Exception as e:
        logger.exception("Error in /summarize endpoint")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/keywords", response_model=KeywordResponse)
def get_keywords(request: KeywordRequest):
    try:
        if not request.articles:
            raise HTTPException(status_code=400, detail="ê¸°ì‚¬ í•˜ë‚˜ ì´ìƒ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

        docs = [article.content for article in request.articles]
        allowed_pos = {"Noun"}
        stopwords = {
            "ì „ë…„", "ì§€ë‚œí•´", "ì˜¬í•´", "ë™ì›”", "í˜„ëŒ€ì°¨", "ê³„íš", "ëª¨ë¹„ìŠ¤", "í¼ì„¼íŠ¸", "ëŒ€ë‹¤", "í•˜ë‹¤", "ê¸°ì•„", "ëŒ€ë¹„", "ê°€ìž¥", "ìžˆë‹¤",
            "ì‚¼ì„±ì „ìž", "SKí•˜ì´ë‹‰ìŠ¤", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "í˜„ëŒ€ì°¨", "ê¸°ì•„", "ì—ì–´ë¡œìŠ¤íŽ˜ì´ìŠ¤", "í•œì „", "ë¡¯ë°", "ë°˜ë©´", "ì‹ í•œì¹´ë“œ", "ì‹±ìŠ¤", "ì†”ì´", "ì§ì ‘", "ì•Œë¦¬", "ì‹ í•œì¹´ë“œ",
            "ìµìŠ¤", "í”„ë ˆ", "ì—ì–´ë¡œ", "ìŠ¤íŽ˜ì´ìŠ¤", "íŒŒì‚¬", "ê¸ˆì„±", "íŠ¸ë¡ ", "ìš°ë¦¬ê¸ˆìœµ",
            "ì…€íŠ¸ë¦¬ì˜¨", "KBê¸ˆìœµ", "NAVER", "HDí˜„ëŒ€ì¤‘ê³µì—…", "ì‹ í•œì§€ì£¼", "í˜„ëŒ€ëª¨ë¹„ìŠ¤", 
            "POSCOí™€ë”©ìŠ¤", "ì‚¼ì„±ë¬¼ì‚°", "ë©”ë¦¬ì¸ ê¸ˆìœµì§€ì£¼", "ê³ ë ¤ì•„ì—°", "ì‚¼ì„±ìƒëª…", "LGí™”í•™",
            "ì‚¼ì„±í™”ìž¬", "SKì´ë…¸ë² ì´ì…˜", "ì‚¼ì„±SDI", "ì¹´ì¹´ì˜¤", "í•œí™”ì—ì–´ë¡œìŠ¤íŽ˜ì´ìŠ¤", 
            "HDí•œêµ­ì¡°ì„ í•´ì–‘", "í•˜ë‚˜ê¸ˆìœµì§€ì£¼", "HMM", "í¬ëž˜í”„í†¤", "HDí˜„ëŒ€ì¼ë ‰íŠ¸ë¦­", 
            "LGì „ìž", "KT&G", "í•œêµ­ì „ë ¥", "SKí…”ë ˆì½¤", "í•œí™”ì˜¤ì…˜", "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°", 
            "ê¸°ì—…ì€í–‰", "LG", "ìš°ë¦¬ê¸ˆìœµì§€ì£¼", "KT", "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "SKìŠ¤í€˜ì–´",
            "ì‚¼ì„±", "ì „ìž", "SK", "í•˜ì´ë‹‰ìŠ¤", "ì—ë„ˆì§€", "ì†”ë£¨ì…˜", "ë°”ì´ì˜¤ë¡œì§ìŠ¤", "í˜„ëŒ€", "ì°¨",
            "KB", "HD", "ì¤‘ê³µì—…", "ì‹ í•œ", "ì§€ì£¼", "ëª¨ë¹„ìŠ¤", 
            "POSCO", "í™€ë”©ìŠ¤", "ë¬¼ì‚°", "ë©”ë¦¬ì¸ ", "ê³ ë ¤", "ì•„ì—°", "ìƒëª…", "í™”í•™", "í™”ìž¬", 
            "ì´ë…¸ë² ì´ì…˜", "SDI", "í•œí™”", "ì—ì–´ë¡œìŠ¤íŽ˜ì´ìŠ¤", "í•œêµ­", "ì¡°ì„ í•´ì–‘", 
            "í•˜ë‚˜", "ì¼ë ‰íŠ¸ë¦­", "ì „ë ¥", "í…”ë ˆì½¤", "ì˜¤ì…˜", "ë‘ì‚°", "ì—ë„ˆë¹Œë¦¬í‹°", "ê¸°ì—…", "ì€í–‰", "ìš°ë¦¬",
            "í“¨ì²˜ì— ", "ìŠ¤í€˜ì–´", "ì‚¼ì „", "í•˜ë‹‰", "ì—”ì†”", "ì—˜ì§€", "ì—˜ì¥", "KIA", "ì…€íŠ¸", "ë„¤ì´ë²„", "í¬ìŠ¤ì½”", 
            "ì‚¼ë¬¼", "ì´ë…¸", "í ìŠ¬ë¼", "ìŠ¤í¬íŠ¸", "ê¸°ì€", "ì¼€ì´í‹°", "ì—ìŠ¤ì¼€ì´", "IBK"
        }

        keywords = [
            word for doc in docs
            for word, tag in okt.pos(doc, stem=True)
            if tag in allowed_pos and len(word) > 1 and word not in stopwords
        ]

        counter = Counter(keywords)
        keyword_items = [KeywordItem(word=word, count=count) for word, count in counter.most_common(10)]
        return KeywordResponse(keywords=keyword_items)

    except Exception as e:
        logger.exception("Error in /keywords endpoint with Okt + POS filtering")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000)